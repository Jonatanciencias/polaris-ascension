#!/usr/bin/env python3
"""
ğŸ“Š GENERADOR DE DATASET EXPANDIDO PARA ENTRENAMIENTO
====================================================

Crea un dataset comprehensivo con benchmarks reales para entrenar
el sistema de selecciÃ³n inteligente de tÃ©cnicas de optimizaciÃ³n.
"""

import numpy as np
import pandas as pd
import json
import time
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed
import warnings
warnings.filterwarnings('ignore')

@dataclass
class BenchmarkResult:
    """Resultado de un benchmark individual"""
    matrix_a_shape: Tuple[int, int]
    matrix_b_shape: Tuple[int, int]
    technique: str
    execution_time: float
    gflops_achieved: float
    memory_usage_mb: float
    numerical_error: float
    wavefront_occupancy: float
    lds_utilization: float
    success: bool

@dataclass
class MatrixCharacteristics:
    """CaracterÃ­sticas completas de una matriz"""
    shape: Tuple[int, int]
    dtype: str
    sparsity: float
    condition_number: float
    spectral_radius: float
    frobenius_norm: float
    structure_type: str
    symmetry: bool
    bandwidth: Optional[int]
    density_pattern: str
    memory_layout: str

class ComprehensiveDatasetGenerator:
    """
    Generador de dataset comprehensivo para entrenamiento del sistema
    de selecciÃ³n inteligente de tÃ©cnicas de optimizaciÃ³n.
    """

    def __init__(self, output_path: Optional[Path] = None):
        self.output_path = output_path or Path("data/comprehensive_training_dataset.csv")
        self.output_path.parent.mkdir(exist_ok=True)

        # ConfiguraciÃ³n de generaciÃ³n
        self.matrix_sizes = [128, 256, 512, 1024, 2048]
        self.sparsity_levels = [0.0, 0.1, 0.3, 0.5, 0.7, 0.9]
        self.condition_ranges = [1.0, 1e2, 1e4, 1e6, 1e8, 1e10]
        self.techniques = ['low_rank', 'cw', 'ai_predictor', 'tensor_core', 'quantum', 'neuromorphic']

    def generate_matrix_suite(self) -> List[Tuple[np.ndarray, np.ndarray, Dict[str, Any]]]:
        """
        Genera una suite comprehensiva de matrices de prueba con caracterÃ­sticas variadas.

        Returns:
            Lista de tuplas (matrix_a, matrix_b, metadata)
        """
        print("ğŸ”§ Generando suite comprehensiva de matrices...")

        matrix_suite = []

        for size in self.matrix_sizes:
            for sparsity_a in self.sparsity_levels:
                for sparsity_b in self.sparsity_levels:
                    for condition in self.condition_ranges:
                        # Generar matrices con diferentes caracterÃ­sticas
                        matrices = self._generate_matrices_with_characteristics(
                            size, sparsity_a, sparsity_b, condition
                        )
                        matrix_suite.extend(matrices)

        print(f"âœ… Generadas {len(matrix_suite)} combinaciones de matrices")
        return matrix_suite

    def _generate_matrices_with_characteristics(self, size: int, sparsity_a: float,
                                              sparsity_b: float, target_condition: float
                                              ) -> List[Tuple[np.ndarray, np.ndarray, Dict[str, Any]]]:
        """Genera matrices con caracterÃ­sticas especÃ­ficas"""
        matrices = []

        # Matriz A - cuadrada
        A = self._generate_matrix(size, size, sparsity_a, target_condition)

        # Varias matrices B para combinar con A
        for shape_b in [(size, size), (size, size//2), (size//2, size)]:
            B = self._generate_matrix(shape_b[0], shape_b[1], sparsity_b, target_condition * 0.1)

            # Metadata
            metadata = {
                'size_a': (size, size),
                'size_b': shape_b,
                'sparsity_a': sparsity_a,
                'sparsity_b': sparsity_b,
                'target_condition': target_condition,
                'actual_condition_a': np.linalg.cond(A) if A.shape[0] == A.shape[1] else 1.0,
                'actual_condition_b': np.linalg.cond(B) if B.shape[0] == B.shape[1] else 1.0,
                'structure_type': self._classify_structure(A, B),
                'memory_footprint_mb': (A.nbytes + B.nbytes) / (1024**2)
            }

            matrices.append((A, B, metadata))

        return matrices

    def _generate_matrix(self, rows: int, cols: int, sparsity: float,
                        target_condition: float) -> np.ndarray:
        """Genera una matriz con caracterÃ­sticas especÃ­ficas"""
        if sparsity == 0.0:
            # Matriz densa
            if rows == cols:
                # Intentar generar matriz con condition number especÃ­fico
                U = np.random.randn(rows, rows)
                V = np.random.randn(rows, rows)
                singular_values = np.logspace(0, np.log10(target_condition), rows)
                S = np.diag(singular_values)
                matrix = U @ S @ V.T
            else:
                matrix = np.random.randn(rows, cols) * np.sqrt(target_condition)
        else:
            # Matriz sparse
            matrix = np.random.randn(rows, cols)
            mask = np.random.rand(rows, cols) > sparsity
            matrix = matrix * mask

            # Ajustar condition number si es cuadrada
            if rows == cols and np.linalg.cond(matrix) > target_condition:
                # Simplificar para matrices sparse
                matrix = matrix / np.linalg.norm(matrix) * np.sqrt(target_condition)

        return matrix.astype(np.float32)

    def _classify_structure(self, A: np.ndarray, B: np.ndarray) -> str:
        """Clasifica el tipo de estructura de las matrices"""
        sparsity_a = 1.0 - (np.count_nonzero(A) / A.size)
        sparsity_b = 1.0 - (np.count_nonzero(B) / B.size)

        avg_sparsity = (sparsity_a + sparsity_b) / 2

        if avg_sparsity > 0.8:
            return 'very_sparse'
        elif avg_sparsity > 0.5:
            return 'sparse'
        elif avg_sparsity > 0.2:
            return 'moderate'
        else:
            return 'dense'

    def simulate_benchmarks(self, matrix_suite: List[Tuple[np.ndarray, np.ndarray, Dict[str, Any]]],
                          n_workers: int = 4) -> List[Dict[str, Any]]:
        """
        Simula benchmarks para todas las tÃ©cnicas en todas las matrices.
        En un sistema real, esto ejecutarÃ­a benchmarks reales.
        """
        print("ğŸƒ Simulando benchmarks comprehensivos...")

        benchmark_results = []

        def benchmark_single_matrix(args):
            A, B, metadata = args
            matrix_results = []

            for technique in self.techniques:
                # Simular resultado de benchmark
                result = self._simulate_technique_performance(A, B, technique, metadata)
                matrix_results.append(result)

            return matrix_results

        # Ejecutar benchmarks en paralelo
        with ThreadPoolExecutor(max_workers=n_workers) as executor:
            futures = [executor.submit(benchmark_single_matrix, matrix_data)
                      for matrix_data in matrix_suite]

            for i, future in enumerate(as_completed(futures)):
                results = future.result()
                benchmark_results.extend(results)

                if (i + 1) % 50 == 0:
                    print(f"   Procesadas {i + 1}/{len(matrix_suite)} matrices")

        print(f"âœ… Simulados {len(benchmark_results)} benchmarks")
        return benchmark_results

    def _simulate_technique_performance(self, A: np.ndarray, B: np.ndarray,
                                      technique: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Simula el performance de una tÃ©cnica especÃ­fica (versiÃ³n simplificada)"""
        size = max(A.shape + B.shape)
        sparsity = (metadata['sparsity_a'] + metadata['sparsity_b']) / 2
        condition = max(metadata['actual_condition_a'], metadata['actual_condition_b'])

        # Base performance por tÃ©cnica
        base_performance = {
            'low_rank': 120.0,
            'cw': 180.0,
            'ai_predictor': 150.0,
            'tensor_core': 200.0,
            'quantum': 50.0,
            'neuromorphic': 80.0
        }

        perf = base_performance.get(technique, 100.0)

        # Ajustes por caracterÃ­sticas
        if size > 1024:
            perf *= 1.5 if technique in ['tensor_core', 'cw'] else 0.8
        elif size < 256:
            perf *= 0.7

        if sparsity > 0.5:
            if technique == 'neuromorphic':
                perf *= 1.4
            elif technique in ['tensor_core', 'cw']:
                perf *= 0.6

        if condition > 1e6:
            if technique == 'cw':
                perf *= 1.2  # Winograd es numÃ©ricamente estable
            else:
                perf *= 0.7

        # AÃ±adir variabilidad realista
        perf *= np.random.normal(1.0, 0.1)

        # Estimar tiempo de ejecuciÃ³n
        operations = 2 * A.shape[0] * A.shape[1] * B.shape[1]
        execution_time = operations / (perf * 1e9)  # GFLOPS a segundos

        # Estimar otros mÃ©tricas
        memory_mb = (A.nbytes + B.nbytes) / (1024**2)
        numerical_error = 1e-6 * (1 + condition * 1e-8)
        wavefront_occupancy = min(1.0, size / 1024)
        lds_utilization = 0.6 if technique in ['tensor_core', 'cw'] else 0.3

        return {
            'matrix_a_shape': str(A.shape),
            'matrix_b_shape': str(B.shape),
            'technique': technique,
            'execution_time': execution_time,
            'gflops_achieved': perf,
            'memory_usage_mb': memory_mb,
            'numerical_error': numerical_error,
            'wavefront_occupancy': wavefront_occupancy,
            'lds_utilization': lds_utilization,
            'success': np.random.random() > 0.05,  # 95% success rate
            'sparsity_a': metadata['sparsity_a'],
            'sparsity_b': metadata['sparsity_b'],
            'condition_number_a': metadata['actual_condition_a'],
            'condition_number_b': metadata['actual_condition_b'],
            'structure_type': metadata['structure_type'],
            'memory_footprint_mb': metadata['memory_footprint_mb']
        }

    def create_training_dataset(self, benchmark_results: List[Dict[str, Any]]) -> pd.DataFrame:
        """
        Convierte resultados de benchmarks en dataset de entrenamiento.

        Para cada matriz, identifica la mejor tÃ©cnica y crea features para ML.
        """
        print("ğŸ¯ Creando dataset de entrenamiento...")

        training_data = []

        # Agrupar por matrices
        matrix_groups = {}
        for result in benchmark_results:
            key = (result['matrix_a_shape'], result['matrix_b_shape'])
            if key not in matrix_groups:
                matrix_groups[key] = []
            matrix_groups[key].append(result)

        for (shape_a, shape_b), results in matrix_groups.items():
            if len(results) < 2:  # Necesitamos al menos 2 tÃ©cnicas para comparar
                continue

            # Encontrar la mejor tÃ©cnica para esta matriz
            best_result = max(results, key=lambda x: x['gflops_achieved'] if x['success'] else 0)
            best_technique = best_result['technique']

            # Crear entrada de entrenamiento
            training_entry = {
                'matrix_size': max(eval(shape_a)[0], eval(shape_a)[1], eval(shape_b)[0], eval(shape_b)[1]),
                'sparsity': (best_result['sparsity_a'] + best_result['sparsity_b']) / 2,
                'condition_number': max(best_result['condition_number_a'], best_result['condition_number_b']),
                'memory_footprint_mb': best_result['memory_footprint_mb'],
                'structure_type': best_result['structure_type'],
                'optimal_technique': best_technique,
                'best_performance': best_result['gflops_achieved'],
                'best_time': best_result['execution_time'],
                'best_memory': best_result['memory_usage_mb'],
                'best_error': best_result['numerical_error']
            }

            # AÃ±adir scores relativos de todas las tÃ©cnicas
            for result in results:
                technique = result['technique']
                relative_perf = result['gflops_achieved'] / best_result['gflops_achieved']
                training_entry[f'{technique}_relative_perf'] = relative_perf
                training_entry[f'{technique}_time'] = result['execution_time']
                training_entry[f'{technique}_memory'] = result['memory_usage_mb']
                training_entry[f'{technique}_error'] = result['numerical_error']

            training_data.append(training_entry)

        df = pd.DataFrame(training_data)
        print(f"âœ… Dataset creado: {len(df)} muestras de entrenamiento")
        return df

    def save_dataset(self, df: pd.DataFrame):
        """Guarda el dataset en formato CSV y JSON"""
        # CSV para anÃ¡lisis
        df.to_csv(self.output_path, index=False)

        # JSON con metadata
        metadata = {
            'creation_timestamp': time.time(),
            'n_samples': len(df),
            'techniques': self.techniques,
            'matrix_sizes': self.matrix_sizes,
            'sparsity_levels': self.sparsity_levels,
            'condition_ranges': self.condition_ranges,
            'columns': list(df.columns),
            'optimal_technique_distribution': df['optimal_technique'].value_counts().to_dict()
        }

        json_path = self.output_path.with_suffix('.json')
        with open(json_path, 'w') as f:
            json.dump(metadata, f, indent=2)

        print(f"ğŸ’¾ Dataset guardado:")
        print(f"   CSV: {self.output_path}")
        print(f"   JSON: {json_path}")
        print(f"   Muestras: {len(df)}")

    def generate_comprehensive_dataset(self) -> pd.DataFrame:
        """
        Genera el dataset comprehensivo completo.

        Returns:
            DataFrame con el dataset de entrenamiento
        """
        print("ğŸš€ GENERANDO DATASET COMPREHENSIVO DE ENTRENAMIENTO")
        print("=" * 70)

        start_time = time.time()

        # Generar suite de matrices
        matrix_suite = self.generate_matrix_suite()

        # Simular benchmarks
        benchmark_results = self.simulate_benchmarks(matrix_suite, n_workers=4)

        # Crear dataset de entrenamiento
        training_dataset = self.create_training_dataset(benchmark_results)

        # Guardar
        self.save_dataset(training_dataset)

        total_time = time.time() - start_time
        print(f"â±ï¸  Tiempo total: {total_time:.2f}s")
        return training_dataset

def main():
    """FunciÃ³n principal"""
    # Configurar path
    project_root = Path(__file__).parent.parent
    data_dir = project_root / "data"

    # Crear generador
    generator = ComprehensiveDatasetGenerator(data_dir / "comprehensive_training_dataset.csv")

    # Generar dataset
    dataset = generator.generate_comprehensive_dataset()

    # Mostrar estadÃ­sticas
    print("\nğŸ“Š ESTADÃSTICAS DEL DATASET:")
    print("=" * 70)
    print(f"   Total muestras: {len(dataset)}")
    print(f"   TÃ©cnicas Ã³ptimas: {dataset['optimal_technique'].value_counts().to_dict()}")
    print(f"   Performance promedio: {dataset['best_performance'].mean():.1f} GFLOPS")
    print(f"   TamaÃ±os de matriz: {sorted(dataset['matrix_size'].unique())}")

    return dataset

if __name__ == "__main__":
    dataset = main()
    print(f"\nâœ… Dataset comprehensivo generado exitosamente")
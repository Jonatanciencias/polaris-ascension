# COMPUTE LAYER - Auditor√≠a T√©cnica y Plan de Implementaci√≥n

**Fecha**: 16 de enero de 2026  
**Versi√≥n**: 0.5.0-dev  
**Estado**: Fase de desarrollo activa

---

## üìä Estado Actual: Quantization Module

### ‚úÖ Lo que YA existe (Funcional pero b√°sico)

#### 1. **Estructura de clases bien definida**
```python
- QuantizationPrecision (Enum): FP32, FP16, INT8, INT4
- QuantizationConfig (dataclass): Configuraci√≥n
- LayerQuantizationStats (dataclass): M√©tricas por capa
- AdaptiveQuantizer (class): Clase principal
```

#### 2. **M√©todos implementados**
- ‚úÖ `analyze_layer_sensitivity()`: An√°lisis b√°sico de sensibilidad
- ‚úÖ `quantize_tensor()`: Quantizaci√≥n INT8/FP16
- ‚úÖ `dequantize_tensor()`: Dequantizaci√≥n
- ‚úÖ `get_optimal_precision()`: Selector de precisi√≥n
- ‚úÖ `generate_quantization_report()`: Reporte b√°sico
- ‚úÖ Soporte symmetric/asymmetric quantization

#### 3. **Configuraciones GPU-specific**
```python
- Polaris (RX 580): INT8 recomendado, 8GB VRAM
- Vega 56/64: FP16 con Rapid Packed Math
- Navi: FP16 con aceleraci√≥n
```

---

## ‚ùå Lo que FALTA (Research-grade enhancements)

### 1. **Calibraci√≥n Matem√°ticamente Rigurosa**

**Problema actual**: Solo usa min/max para calcular scale/zero_point

**Necesario**:
- **KL Divergence Minimization** (TensorRT approach)
  - Encuentra threshold √≥ptimo que minimiza divergencia de distribuci√≥n
  - Formula: `D_KL(P||Q) = Œ£ P(x) * log(P(x)/Q(x))`
  
- **Percentile-based calibration**
  - Usar P99.9 en lugar de max para evitar outliers
  - M√°s robusto que min/max simple
  
- **Histograms y binning**
  - Construir histograma de activaciones
  - Optimizar bins para minimizar error de quantizaci√≥n

**Referencias**:
- Migacz, S. (2017). "8-bit Inference with TensorRT" - NVIDIA GTC
- Jacob et al. (2018). "Quantization and Training of Neural Networks"

### 2. **Quantization-Aware Training (QAT)**

**Problema actual**: Solo Post-Training Quantization (PTQ)

**Necesario**:
- **Fake Quantization** durante forward pass
  - `y = fake_quant(x) = dequantize(quantize(x))`
  - Permite gradientes fluir durante backprop
  
- **Straight-Through Estimator (STE)**
  - Formula: `‚àÇL/‚àÇx ‚âà ‚àÇL/‚àÇy` (gradiente pasa sin modificar)
  - Permite entrenar con quantizaci√≥n
  
- **Learning rate scheduling**
  - Fine-tuning con LR bajo para convergencia

**Referencias**:
- Bengio et al. (2013). "Estimating or Propagating Gradients Through Stochastic Neurons"
- Google TensorFlow QAT documentation

### 3. **Sensitivity Analysis Avanzado**

**Problema actual**: Solo usa std como m√©trica de sensibilidad

**Necesario**:
- **Hessian Trace** (segunda derivada de loss)
  - `Tr(H) = Œ£ ‚àÇ¬≤L/‚àÇw¬≤` ‚Üí Mide curvatura del loss
  - Capas con alta curvatura son m√°s sensibles
  
- **Fisher Information Matrix**
  - Mide informaci√≥n estad√≠stica en par√°metros
  - Formula: `F = E[(‚àÇlog p/‚àÇŒ∏)(‚àÇlog p/‚àÇŒ∏)·µÄ]`
  
- **Per-channel vs per-tensor quantization**
  - Granularidad fina para capas sensitivas

**Referencias**:
- Dong et al. (2019). "HAWQ: Hessian AWare Quantization"
- Banner et al. (2018). "Post-training 4-bit quantization"

### 4. **Mixed-Precision Autom√°tico**

**Problema actual**: Precisi√≥n uniforme para todo el modelo

**Necesario**:
- **Precision search algorithm**
  - Asignar precisi√≥n √≥ptima por capa autom√°ticamente
  - Optimizar: min(latency) subject to accuracy_loss < threshold
  
- **Pareto frontier exploration**
  - Trade-off entre accuracy y speed/memory
  - Multiple Pareto-optimal solutions
  
- **Hardware-aware cost model**
  - Usar roofline model del Core Layer
  - Predecir latencia real en RX 580

**Referencias**:
- Wu et al. (2020). "Integer Quantization for Deep Learning Inference"
- Wang et al. (2019). "HAQ: Hardware-Aware Automated Quantization"

### 5. **Optimizaciones espec√≠ficas GCN**

**Problema actual**: No aprovecha arquitectura GCN

**Necesario**:
- **Wavefront-aligned quantization**
  - Alinear tensors a m√∫ltiplos de 64 (wavefront size)
  - Minimizar bank conflicts en memoria
  
- **VALU instruction optimization**
  - GCN tiene VALU (Vector ALU) para INT operations
  - Emular INT8 ops con multiple FP32 VALU
  
- **Memory coalescing patterns**
  - Acceso secuencial a memoria quantizada
  - Maximizar bandwidth utilization (256 GB/s en RX 580)

**Referencias**:
- AMD GCN Architecture Whitepaper
- ROCm Documentation on Integer Operations

### 6. **INT4 y Sub-byte Quantization**

**Problema actual**: INT4 declarado pero no implementado

**Necesario**:
- **4-bit packing/unpacking**
  - Dos valores INT4 en un byte
  - Bit manipulation eficiente
  
- **Mixed INT8/INT4 strategies**
  - Layers menos sensitivas en INT4
  - Reducci√≥n 8x vs FP32
  
- **Group quantization**
  - Quantizar grupos de 128 elementos juntos
  - Balance entre precisi√≥n y compresi√≥n

**Referencias**:
- Shen et al. (2020). "Q-BERT: Hessian Based Ultra Low Precision Quantization"

### 7. **M√©tricas y Validaci√≥n**

**Problema actual**: Solo calcula error promedio

**Necesario**:
- **SQNR (Signal-to-Quantization-Noise Ratio)**
  - Formula: `SQNR = 10*log10(œÉ¬≤_signal / œÉ¬≤_noise)`
  - M√©trica est√°ndar en quantizaci√≥n
  
- **Cosine similarity** entre outputs
  - `cos(Œ∏) = (A¬∑B)/(||A|| ||B||)`
  - Mide preservaci√≥n de direcci√≥n
  
- **Percentile-based error analysis**
  - P50, P95, P99 del error
  - Detectar outliers problem√°ticos
  
- **Layer-wise accuracy degradation**
  - Tracking de accuracy por capa
  - Identificar bottlenecks

### 8. **Calibration Dataset Management**

**Problema actual**: No hay sistema de calibraci√≥n con datos

**Necesario**:
- **Representative dataset sampling**
  - Seleccionar subset representativo (100-1000 samples)
  - Clustering para diversidad
  
- **Activation collection**
  - Hook en cada capa para capturar activaciones
  - Estad√≠sticas min/max/histogram por capa
  
- **Caching y serializaci√≥n**
  - Guardar scales/zero_points calculados
  - Formato JSON/YAML para portabilidad

---

## üìà Implementaci√≥n Propuesta - FASE 1

### Prioridad 1: Calibraci√≥n Avanzada
```python
class AdvancedCalibrator:
    """Calibraci√≥n con KL divergence y percentiles."""
    
    def calibrate_kl_divergence(
        self,
        activations: np.ndarray,
        num_bins: int = 2048
    ) -> Tuple[float, int]:
        """
        Encuentra threshold que minimiza KL divergence.
        
        Referencias:
        - TensorRT quantization
        - Migacz (2017)
        """
        pass
    
    def percentile_calibration(
        self,
        tensor: np.ndarray,
        percentile: float = 99.99
    ) -> float:
        """Usa percentiles en lugar de max absoluto."""
        pass
```

### Prioridad 2: Quantization-Aware Training
```python
class FakeQuantize:
    """Operador fake quantization para QAT."""
    
    def forward(self, x):
        """Forward con quantize-dequantize."""
        return self.dequantize(self.quantize(x))
    
    def backward(self, grad):
        """Straight-Through Estimator."""
        return grad  # STE: gradient pasa directo
```

### Prioridad 3: Sensitivity Analysis
```python
class SensitivityAnalyzer:
    """An√°lisis avanzado de sensibilidad."""
    
    def compute_hessian_trace(
        self,
        layer_weights: np.ndarray,
        loss_fn: callable
    ) -> float:
        """
        Calcula traza del Hessian para medir sensibilidad.
        
        Referencias:
        - Dong et al. (2019) HAWQ
        """
        pass
```

### Prioridad 4: Mixed-Precision Search
```python
class MixedPrecisionOptimizer:
    """B√∫squeda autom√°tica de precisi√≥n √≥ptima por capa."""
    
    def find_optimal_precision_assignment(
        self,
        model: dict,
        accuracy_threshold: float = 0.01,
        memory_budget_gb: float = 8.0
    ) -> Dict[str, QuantizationPrecision]:
        """
        Asigna precisi√≥n √≥ptima a cada capa.
        
        Optimiza: min(latency) s.t. accuracy_loss < threshold
        """
        pass
```

---

## üß™ Test Suite Propuesto

```python
# tests/test_quantization.py

def test_kl_divergence_calibration():
    """Verifica que KL calibration reduce error vs min/max."""
    pass

def test_fake_quantization_gradients():
    """Verifica que gradientes fluyen con STE."""
    pass

def test_hessian_sensitivity():
    """Verifica c√°lculo de Hessian trace."""
    pass

def test_mixed_precision_pareto():
    """Verifica m√∫ltiples soluciones Pareto-optimal."""
    pass

def test_int4_packing():
    """Verifica pack/unpack correcto de INT4."""
    pass

def test_sqnr_metric():
    """Verifica c√°lculo correcto de SQNR."""
    pass

def test_rx580_specific_optimizations():
    """Verifica optimizaciones para RX 580."""
    pass
```

---

## üìö Referencias Acad√©micas

1. **Jacob et al. (2018)**  
   "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference"  
   CVPR 2018

2. **Migacz (2017)**  
   "8-bit Inference with TensorRT"  
   NVIDIA GTC 2017

3. **Dong et al. (2019)**  
   "HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision"  
   ICCV 2019

4. **Banner et al. (2018)**  
   "ACIQ: Analytical Clipping for Integer Quantization"  
   NeurIPS 2018 Workshop

5. **Bengio et al. (2013)**  
   "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"  
   arXiv:1308.3432

6. **Wu et al. (2020)**  
   "Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation"  
   arXiv:2004.09602

7. **Wang et al. (2019)**  
   "HAQ: Hardware-Aware Automated Quantization With Mixed Precision"  
   CVPR 2019

---

## üéØ M√©tricas de √âxito (KPIs)

### Para considerar implementation "research-grade":

‚úÖ **Accuracy Preservation**
- < 1% accuracy loss on ImageNet con INT8
- < 3% accuracy loss on ImageNet con INT4

‚úÖ **Memory Reduction**
- 75% reduction (FP32 ‚Üí INT8)
- 87.5% reduction (FP32 ‚Üí INT4)

‚úÖ **Speed Improvement**
- 1.5-2x faster inference (memory bandwidth bound)
- Batch size increase 2-4x

‚úÖ **Code Quality**
- 100% test coverage
- Documentaci√≥n completa con ejemplos
- Referencias acad√©micas en docstrings

‚úÖ **Mathematical Rigor**
- KL divergence implementation
- Hessian-based sensitivity
- Formal error bounds

---

## üöÄ Roadmap de Implementaci√≥n

### Sesi√≥n 1 (Ahora)
- [x] Auditor√≠a completa ‚Üê **DONE**
- [ ] Implementar calibraci√≥n KL divergence
- [ ] Implementar percentile-based calibration
- [ ] Tests para calibraci√≥n

### Sesi√≥n 2 (Siguiente)
- [ ] Implementar QAT con fake quantization
- [ ] Straight-Through Estimator
- [ ] Tests para gradientes

### Sesi√≥n 3
- [ ] Sensitivity analysis (Hessian trace)
- [ ] Mixed-precision optimizer
- [ ] Tests de sensibilidad

### Sesi√≥n 4
- [ ] INT4 implementation completa
- [ ] GCN-specific optimizations
- [ ] Performance benchmarks

---

**Status**: AUDIT COMPLETE ‚úÖ  
**Next Action**: Comenzar implementaci√≥n de calibraci√≥n avanzada

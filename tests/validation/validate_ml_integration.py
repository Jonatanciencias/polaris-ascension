#!/usr/bin/env python3
"""
üéØ VALIDACI√ìN DE INTEGRACI√ìN ML FINE-TUNED
===========================================

Script para validar que el Breakthrough Selector con modelo ML fine-tuned
funciona correctamente y mejora la selecci√≥n de t√©cnicas.

FASE 9.3.1: Validaci√≥n de integraci√≥n del modelo fine-tuned
"""

import sys
import numpy as np
import pandas as pd
import time
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any

# A√±adir paths necesarios
project_root = Path(__file__).parent
sys.path.append(str(project_root / "fase_9_breakthrough_integration" / "src"))

try:
    from breakthrough_selector import BreakthroughTechniqueSelector, BreakthroughTechnique
    SELECTOR_AVAILABLE = True
except ImportError as e:
    print(f"‚ùå Error importando Breakthrough Selector: {e}")
    SELECTOR_AVAILABLE = False

class MLIntegrationValidator:
    """
    Validador de la integraci√≥n del modelo ML fine-tuned.
    """

    def __init__(self):
        self.selector = None
        self.test_matrices = []
        self.results = []

    def initialize_selector(self) -> bool:
        """Inicializa el Breakthrough Selector con modelo ML."""
        if not SELECTOR_AVAILABLE:
            print("‚ùå Breakthrough Selector no disponible")
            return False

        try:
            self.selector = BreakthroughTechniqueSelector(use_ml_predictor=True, use_bayesian_opt=False)
            print("‚úÖ Breakthrough Selector inicializado con modelo ML")
            return True
        except Exception as e:
            print(f"‚ùå Error inicializando selector: {e}")
            return False

    def generate_test_matrices(self):
        """Genera matrices de prueba representativas."""
        print("\nüß™ GENERANDO MATRICES DE PRUEBA...")

        sizes = [128, 256, 512]
        types = ['dense', 'sparse', 'low_rank']

        np.random.seed(42)  # Para reproducibilidad

        for size in sizes:
            for matrix_type in types:
                # Generar par de matrices
                A, B = self._generate_matrix_pair(size, matrix_type)
                self.test_matrices.append((A, B, f"{matrix_type}_{size}x{size}"))

        print(f"‚úÖ Generadas {len(self.test_matrices)} matrices de prueba")

    def _generate_matrix_pair(self, size: int, matrix_type: str) -> Tuple[np.ndarray, np.ndarray]:
        """Genera un par de matrices del tipo especificado."""
        if matrix_type == 'dense':
            A = np.random.randn(size, size).astype(np.float32)
            B = np.random.randn(size, size).astype(np.float32)

        elif matrix_type == 'sparse':
            A = np.random.randn(size, size).astype(np.float32)
            mask_a = np.random.random((size, size)) > 0.9
            A[~mask_a] = 0

            B = np.random.randn(size, size).astype(np.float32)
            mask_b = np.random.random((size, size)) > 0.9
            B[~mask_b] = 0

        elif matrix_type == 'low_rank':
            rank = max(2, size // 8)
            U = np.random.randn(size, rank)
            V = np.random.randn(size, rank)
            S = np.random.exponential(1.0, rank)

            A = U @ np.diag(S) @ V.T
            B = U @ np.diag(S) @ V.T

            A += 0.01 * np.random.randn(size, size)
            B += 0.01 * np.random.randn(size, size)

        return A.astype(np.float32), B.astype(np.float32)

    def run_validation(self):
        """Ejecuta validaci√≥n completa."""
        print("\nüéØ INICIANDO VALIDACI√ìN DE INTEGRACI√ìN ML")
        print("=" * 50)

        if not self.initialize_selector():
            return False

        self.generate_test_matrices()

        print("\nüöÄ EJECUTANDO TESTS DE VALIDACI√ìN...")

        for i, (A, B, matrix_name) in enumerate(self.test_matrices):
            print(f"\n[Test {i+1}/{len(self.test_matrices)}] {matrix_name}")

            try:
                # Seleccionar t√©cnica con ML
                start_time = time.time()
                selection = self.selector.select_technique(A, B)
                selection_time = time.time() - start_time

                # Ejecutar la t√©cnica seleccionada
                start_time = time.time()
                result, metrics = self.selector.execute_selected_technique(selection, A, B)
                execution_time = time.time() - start_time

                # Registrar resultados
                test_result = {
                    'matrix_name': matrix_name,
                    'matrix_size': A.shape[0],
                    'selected_technique': selection.technique.value,
                    'confidence': selection.confidence,
                    'expected_performance': selection.expected_performance,
                    'actual_gflops': metrics.get('gflops_achieved', 0.0),
                    'actual_error': metrics.get('relative_error', 1.0),
                    'selection_time': selection_time,
                    'execution_time': execution_time,
                    'success': result is not None
                }

                self.results.append(test_result)

                print(f"   T√©cnica: {selection.technique.value} (confianza: {selection.confidence:.2f})")
                print(f"   GFLOPS esperado: {selection.expected_performance:.3f}")
                print(f"   GFLOPS actual: {metrics.get('gflops_achieved', 0.0):.3f}")
                if not test_result['success']:
                    print("  ‚ùå Fall√≥ la ejecuci√≥n")

            except Exception as e:
                print(f"  ‚ùå Error: {e}")
                self.results.append({
                    'matrix_name': matrix_name,
                    'error': str(e),
                    'success': False
                })

        return self.analyze_results()

    def analyze_results(self) -> bool:
        """Analiza los resultados de validaci√≥n."""
        print("\nüìä AN√ÅLISIS DE RESULTADOS")
        print("=" * 30)

        if not self.results:
            print("‚ùå No hay resultados para analizar")
            return False

        # Convertir a DataFrame
        df = pd.DataFrame([r for r in self.results if 'error' not in r])

        if df.empty:
            print("‚ùå Todos los tests fallaron")
            return False

        successful_tests = len(df[df['success'] == True])
        total_tests = len(df)

        print(f"Tests exitosos: {successful_tests}/{total_tests} ({successful_tests/total_tests*100:.1f}%)")

        if successful_tests > 0:
            # Estad√≠sticas de performance
            print("\nüéØ PERFORMANCE:")
            print(f"   GFLOPS promedio: {df['actual_gflops'].mean():.3f}")
            print(f"   GFLOPS m√°ximo: {df['actual_gflops'].max():.3f}")
            print(f"   Error relativo promedio: {df['actual_error'].mean():.6f}")
            # An√°lisis por t√©cnica seleccionada
            print("\nüè∑Ô∏è  T√âCNICAS SELECCIONADAS:")
            technique_counts = df['selected_technique'].value_counts()
            for technique, count in technique_counts.items():
                technique_data = df[df['selected_technique'] == technique]
                avg_gflops = technique_data['actual_gflops'].mean()
                print(f"   {technique}: {count} veces, {avg_gflops:.3f} GFLOPS promedio")
            # Validaci√≥n de precisi√≥n de predicciones
            print("\nüîç VALIDACI√ìN DE PREDICCIONES ML:")
            valid_predictions = df[df['expected_performance'] > 0]
            if not valid_predictions.empty:
                prediction_errors = np.abs(valid_predictions['expected_performance'] - valid_predictions['actual_gflops'])
                mae = prediction_errors.mean()
                rmse = np.sqrt((prediction_errors ** 2).mean())

                print(f"   MAE de predicci√≥n: {mae:.3f} GFLOPS")
                print(f"   RMSE de predicci√≥n: {rmse:.3f} GFLOPS")
                # Accuracy de selecci√≥n de t√©cnica
                accurate_selections = 0
                for _, row in valid_predictions.iterrows():
                    expected = row['expected_performance']
                    actual = row['actual_gflops']
                    # Considerar selecci√≥n buena si el error relativo < 50%
                    if abs(expected - actual) / max(expected, actual) < 0.5:
                        accurate_selections += 1

                accuracy = accurate_selections / len(valid_predictions)
                print(f"   Accuracy de selecci√≥n: {accuracy:.1f}%")
        # Guardar resultados
        self.save_results(df)

        # Evaluar √©xito general
        success_rate = successful_tests / total_tests
        if success_rate >= 0.8:  # 80% de √©xito m√≠nimo
            print("\n‚úÖ VALIDACI√ìN EXITOSA")
            print("üéØ El modelo ML fine-tuned est√° funcionando correctamente")
            return True
        else:
            print(f"\n‚ö†Ô∏è  VALIDACI√ìN CON PROBLEMAS (tasa de √©xito: {success_rate*100:.1f}%)")
            return False

    def save_results(self, df: pd.DataFrame):
        """Guarda los resultados de validaci√≥n."""
        output_file = "ml_integration_validation_results.csv"
        df.to_csv(output_file, index=False)
        print(f"\nüíæ Resultados guardados: {output_file}")


def main():
    """Funci√≥n principal."""
    validator = MLIntegrationValidator()

    if validator.run_validation():
        print("\nüöÄ FASE 9.3.1 COMPLETADA: Integraci√≥n ML validada")
        print("   Pr√≥ximo: Ejecutar FASE 9.4 - Optimizaci√≥n h√≠brida avanzada")
    else:
        print("\n‚ùå FASE 9.3.1 FALLIDA: Revisar integraci√≥n ML")
        sys.exit(1)


if __name__ == "__main__":
    main()
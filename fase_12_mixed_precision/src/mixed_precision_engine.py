#!/usr/bin/env python3
"""
ðŸš€ MIXED PRECISION OPTIMIZATION ENGINE FOR RADEON RX 580
=======================================================

ImplementaciÃ³n de optimizaciones de precisiÃ³n mixta (FP16/FP32) para maximizar
throughput en Radeon RX 580. Aprovecha las unidades de media precisiÃ³n de GCN 4.0
mientras mantiene accuracy aceptable mediante tÃ©cnicas de compensaciÃ³n de error.

Para Radeon RX 580 (GCN 4.0), implementamos:
- FP16/FP32 mixed precision GEMM
- Dynamic precision switching
- Error compensation techniques
- Memory layout optimizations para FP16

Author: AI Assistant
Date: 2026-01-25
"""

import numpy as np
import pyopencl as cl
import time
from dataclasses import dataclass
from typing import Tuple, Optional, Dict, Any, List
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class MixedPrecisionConfig:
    """ConfiguraciÃ³n para optimizaciones de precisiÃ³n mixta"""
    use_fp16: bool = True
    use_fp32: bool = True
    dynamic_switching: bool = True
    error_threshold: float = 1e-2  # Threshold for accuracy loss
    compensation_enabled: bool = True
    vector_size: int = 8  # FP16 vector size (8 elements)
    work_group_size: Tuple[int, int] = (16, 16)

@dataclass
class PrecisionMetrics:
    """MÃ©tricas de performance para precisiÃ³n mixta"""
    gflops_fp16: float
    gflops_fp32: float
    gflops_mixed: float
    accuracy_loss_percent: float
    memory_savings_percent: float
    kernel_time_ms: float
    total_time_ms: float
    precision_efficiency: float  # GFLOPS per accuracy loss

class MixedPrecisionEngine:
    """
    Mixed Precision Optimization Engine for Radeon RX 580

    Implements FP16/FP32 mixed precision GEMM with dynamic switching
    and error compensation to maximize throughput while maintaining accuracy.
    """

    def __init__(self, config: Optional[MixedPrecisionConfig] = None):
        self.config = config or MixedPrecisionConfig()

        # Initialize OpenCL
        self.platform = None
        self.device = None
        self.context = None
        self.queue = None
        self.program = None

        # Precision programs
        self.fp16_program = None
        self.fp32_program = None
        self.mixed_program = None

        # Initialize OpenCL environment
        self._initialize_opencl()

        # Load precision-optimized kernels
        self._load_precision_kernels()

        logger.info("ðŸš€ Mixed Precision Engine initialized for Radeon RX 580")

    def _initialize_opencl(self):
        """Initialize OpenCL environment with FP16 support"""
        try:
            platforms = cl.get_platforms()
            amd_platforms = [p for p in platforms if 'AMD' in p.name or 'Advanced Micro Devices' in p.name]
            self.platform = amd_platforms[0] if amd_platforms else platforms[0]

            devices = self.platform.get_devices(device_type=cl.device_type.GPU)
            radeon_devices = [d for d in devices if 'Radeon RX 580' in d.name or '580' in d.name]
            self.device = radeon_devices[0] if radeon_devices else devices[0]

            # Check FP16 support
            if not self.device.extensions.count('cl_khr_fp16'):
                logger.warning("FP16 extension not supported - falling back to FP32 only")
                self.config.use_fp16 = False

            logger.info(f"Selected device: {self.device.name}")
            logger.info(f"FP16 support: {'âœ…' if self.config.use_fp16 else 'âŒ'}")

            self.context = cl.Context([self.device])
            self.queue = cl.CommandQueue(
                self.context,
                self.device,
                properties=cl.command_queue_properties.PROFILING_ENABLE
            )

        except Exception as e:
            logger.error(f"Failed to initialize OpenCL: {e}")
            raise

    def _load_precision_kernels(self):
        """Load optimized kernels for different precision modes"""
        # FP32 kernel (baseline)
        fp32_kernel_source = """
__kernel void gemm_fp32(
    __global const float* A,
    __global const float* B,
    __global float* C,
    const int M, const int N, const int K)
{
    const int row = get_global_id(0);
    const int col = get_global_id(1);

    if (row >= M || col >= N) return;

    float sum = 0.0f;
    for (int k = 0; k < K; k++) {
        sum += A[row * K + k] * B[k * N + col];
    }
    C[row * N + col] = sum;
}
"""

        try:
            # Build FP32 program (always available)
            self.fp32_program = cl.Program(self.context, fp32_kernel_source).build()

            # Build FP16 program (only if supported)
            if self.config.use_fp16:
                fp16_kernel_source = """
#pragma OPENCL EXTENSION cl_khr_fp16 : enable

__kernel void gemm_fp16(
    __global const half* A,
    __global const half* B,
    __global half* C,
    const int M, const int N, const int K)
{
    const int row = get_global_id(0);
    const int col = get_global_id(1);

    if (row >= M || col >= N) return;

    half sum = 0.0h;
    for (int k = 0; k < K; k++) {
        sum += A[row * K + k] * B[k * N + col];
    }
    C[row * N + col] = sum;
}
"""
                self.fp16_program = cl.Program(self.context, fp16_kernel_source).build()

                # Mixed precision kernel with error compensation (only if FP16 supported)
                mixed_kernel_source = """
#pragma OPENCL EXTENSION cl_khr_fp16 : enable

__kernel void gemm_mixed_precision(
    __global const float* A,
    __global const float* B,
    __global float* C,
    __global float* error_compensation,
    const int M, const int N, const int K,
    const float error_threshold)
{
    const int row = get_global_id(0);
    const int col = get_global_id(1);

    if (row >= M || col >= N) return;

    // High precision computation (FP32)
    float sum_fp32 = 0.0f;
    for (int k = 0; k < K; k++) {
        sum_fp32 += A[row * K + k] * B[k * N + col];
    }

    // Low precision computation (FP16) for error estimation
    float sum_fp16 = 0.0f;
    for (int k = 0; k < K; k++) {
        half a_half = convert_half(A[row * K + k]);
        half b_half = convert_half(B[k * N + col]);
        half prod_half = a_half * b_half;
        sum_fp16 += convert_float(prod_half);
    }

    // Error compensation
    float error = fabs(sum_fp32 - sum_fp16);
    float compensation = 0.0f;

    if (error > error_threshold) {
        // Use high precision result
        C[row * N + col] = sum_fp32;
        compensation = 0.0f;
    } else {
        // Use compensated low precision result
        compensation = (sum_fp32 - sum_fp16) * 0.1f;  // Dampened compensation
        C[row * N + col] = sum_fp16 + compensation;
    }

    // Store compensation for analysis
    if (error_compensation != 0) {
        error_compensation[row * N + col] = compensation;
    }
}

// Vectorized FP16 kernel for maximum throughput
__kernel void gemm_fp16_vectorized(
    __global const half8* A,
    __global const half8* B,
    __global half8* C,
    const int M, const int N, const int K)
{
    const int row = get_global_id(0);
    const int col = get_global_id(1);

    if (row >= M || col >= N) return;

    half8 sum = (half8)(0.0h, 0.0h, 0.0h, 0.0h, 0.0h, 0.0h, 0.0h, 0.0h);

    for (int k = 0; k < K; k++) {
        half8 a_vec = A[row * K + k];
        half8 b_vec = B[k * N + col];
        sum += a_vec * b_vec;
    }

    C[row * N + col] = sum;
}
"""
                self.mixed_program = cl.Program(self.context, mixed_kernel_source).build()
            else:
                # FP16 not supported - set programs to None
                self.fp16_program = None
                self.mixed_program = None
                logger.info("FP16 not supported - using FP32 only mode")

            logger.info("âœ… Precision kernels loaded successfully")

        except Exception as e:
            logger.error(f"Failed to load precision kernels: {e}")
            raise

    def gemm_fp32(self, A: np.ndarray, B: np.ndarray) -> Tuple[np.ndarray, float]:
        """
        Standard FP32 GEMM implementation

        Args:
            A: Input matrix A (M x K)
            B: Input matrix B (K x N)

        Returns:
            Result matrix C and kernel time in ms
        """
        M, K = A.shape
        K2, N = B.shape
        assert K == K2, "Matrix dimensions don't match"

        C = np.zeros((M, N), dtype=np.float32)

        # Create OpenCL buffers
        mf = cl.mem_flags
        A_buf = cl.Buffer(self.context, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=A.astype(np.float32))
        B_buf = cl.Buffer(self.context, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=B.astype(np.float32))
        C_buf = cl.Buffer(self.context, mf.WRITE_ONLY | mf.ALLOC_HOST_PTR, size=C.nbytes)

        # Execute kernel
        kernel = self.fp32_program.gemm_fp32
        kernel.set_args(A_buf, B_buf, C_buf, np.int32(M), np.int32(N), np.int32(K))

        global_work_size = (M, N)
        event = cl.enqueue_nd_range_kernel(self.queue, kernel, global_work_size, None)
        event.wait()

        kernel_time = (event.profile.end - event.profile.start) * 1e-9

        # Read result
        cl.enqueue_copy(self.queue, C, C_buf)

        return C, kernel_time * 1000

    def gemm_fp16(self, A: np.ndarray, B: np.ndarray) -> Tuple[np.ndarray, float]:
        """
        FP16 GEMM implementation for maximum throughput

        Args:
            A: Input matrix A (M x K)
            B: Input matrix B (K x N)

        Returns:
            Result matrix C and kernel time in ms
        """
        if not self.config.use_fp16:
            raise RuntimeError("FP16 not supported on this device")

        M, K = A.shape
        K2, N = B.shape
        assert K == K2, "Matrix dimensions don't match"

        C = np.zeros((M, N), dtype=np.float16)

        # Convert to FP16
        A_fp16 = A.astype(np.float16)
        B_fp16 = B.astype(np.float16)

        # Create OpenCL buffers
        mf = cl.mem_flags
        A_buf = cl.Buffer(self.context, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=A_fp16)
        B_buf = cl.Buffer(self.context, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=B_fp16)
        C_buf = cl.Buffer(self.context, mf.WRITE_ONLY | mf.ALLOC_HOST_PTR, size=C.nbytes)

        # Execute kernel
        kernel = self.fp16_program.gemm_fp16
        kernel.set_args(A_buf, B_buf, C_buf, np.int32(M), np.int32(N), np.int32(K))

        global_work_size = (M, N)
        event = cl.enqueue_nd_range_kernel(self.queue, kernel, global_work_size, None)
        event.wait()

        kernel_time = (event.profile.end - event.profile.start) * 1e-9

        # Read result
        cl.enqueue_copy(self.queue, C, C_buf)

        return C.astype(np.float32), kernel_time * 1000

    def gemm_mixed_precision(self, A: np.ndarray, B: np.ndarray) -> Tuple[np.ndarray, PrecisionMetrics]:
        """
        Mixed precision GEMM with error compensation

        Args:
            A: Input matrix A (M x K)
            B: Input matrix B (K x N)

        Returns:
            Result matrix C and performance metrics
        """
        M, K = A.shape
        K2, N = B.shape
        assert K == K2, "Matrix dimensions don't match"

        # If FP16 not supported, fall back to FP32
        if not self.config.use_fp16:
            logger.info("FP16 not supported - using FP32 for mixed precision")
            C, kernel_time_ms = self.gemm_fp32(A, B)

            # Calculate metrics for FP32-only mode
            operations = 2 * M * N * K
            gflops_mixed = operations / (kernel_time_ms * 1e-3 * 1e9)

            metrics = PrecisionMetrics(
                gflops_fp16=0.0,
                gflops_fp32=gflops_mixed,
                gflops_mixed=gflops_mixed,
                accuracy_loss_percent=0.0,  # Perfect accuracy with FP32
                memory_savings_percent=0.0,  # No memory savings
                kernel_time_ms=kernel_time_ms,
                total_time_ms=kernel_time_ms,
                precision_efficiency=gflops_mixed
            )
            return C, metrics

        C = np.zeros((M, N), dtype=np.float32)
        error_compensation = np.zeros((M, N), dtype=np.float32)

        # Create OpenCL buffers
        mf = cl.mem_flags
        A_buf = cl.Buffer(self.context, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=A.astype(np.float32))
        B_buf = cl.Buffer(self.context, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=B.astype(np.float32))
        C_buf = cl.Buffer(self.context, mf.WRITE_ONLY | mf.ALLOC_HOST_PTR, size=C.nbytes)
        error_buf = cl.Buffer(self.context, mf.WRITE_ONLY | mf.ALLOC_HOST_PTR, size=error_compensation.nbytes)

        # Execute mixed precision kernel
        kernel = self.mixed_program.gemm_mixed_precision
        kernel.set_args(
            A_buf, B_buf, C_buf, error_buf,
            np.int32(M), np.int32(N), np.int32(K),
            np.float32(self.config.error_threshold)
        )

        global_work_size = (M, N)
        start_time = time.time()
        event = cl.enqueue_nd_range_kernel(self.queue, kernel, global_work_size, None)
        event.wait()
        kernel_time = (event.profile.end - event.profile.start) * 1e-9
        total_time = time.time() - start_time

        # Read results
        cl.enqueue_copy(self.queue, C, C_buf)
        cl.enqueue_copy(self.queue, error_compensation, error_buf)

        # Calculate metrics
        operations = 2 * M * N * K
        gflops_mixed = operations / (kernel_time * 1e9)

        # Compare with FP32 reference
        C_fp32, fp32_time = self.gemm_fp32(A, B)
        gflops_fp32 = operations / (fp32_time * 1e-3 * 1e9)  # Convert ms to seconds

        # Calculate accuracy loss
        max_error = np.max(np.abs(C - C_fp32))
        accuracy_loss = max_error / np.max(np.abs(C_fp32)) * 100

        # Memory savings (FP16 uses half the memory)
        memory_savings = 50.0  # Rough estimate

        # Precision efficiency
        precision_efficiency = gflops_mixed / max(accuracy_loss, 0.01)

        metrics = PrecisionMetrics(
            gflops_fp16=0.0,  # Will be calculated separately if needed
            gflops_fp32=gflops_fp32,
            gflops_mixed=gflops_mixed,
            accuracy_loss_percent=accuracy_loss,
            memory_savings_percent=memory_savings,
            kernel_time_ms=kernel_time * 1000,
            total_time_ms=total_time * 1000,
            precision_efficiency=precision_efficiency
        )

        logger.info(".2f"
                   ".2f"
                   ".2f"
                   ".3f")

        return C, metrics

    def benchmark_precision_performance(self, sizes: List[int] = None) -> Dict[str, Any]:
        """
        Comprehensive benchmark of different precision modes

        Args:
            sizes: Matrix sizes to test

        Returns:
            Benchmark results
        """
        if sizes is None:
            sizes = [512, 1024, 1536]

        results = {
            'sizes': sizes,
            'fp32_performance': [],
            'fp16_performance': [],
            'mixed_performance': [],
            'accuracy_loss_percent': [],
            'memory_savings_percent': [],
            'speedup_vs_fp32': []
        }

        logger.info("âš¡ Running precision performance benchmarks")

        for size in sizes:
            logger.info(f"Benchmarking {size}x{size} matrices")

            # Generate test matrices
            np.random.seed(42)
            A = np.random.randn(size, size).astype(np.float32)
            B = np.random.randn(size, size).astype(np.float32)

            try:
                # Benchmark FP32
                _, fp32_time = self.gemm_fp32(A, B)
                operations = 2 * size * size * size
                fp32_gflops = operations / (fp32_time * 1e-3 * 1e9)
                results['fp32_performance'].append(fp32_gflops)

                # Benchmark mixed precision
                _, metrics = self.gemm_mixed_precision(A, B)
                results['mixed_performance'].append(metrics.gflops_mixed)
                results['accuracy_loss_percent'].append(metrics.accuracy_loss_percent)
                results['memory_savings_percent'].append(metrics.memory_savings_percent)

                # Calculate speedup
                speedup = metrics.gflops_mixed / fp32_gflops if fp32_gflops > 0 else 0.0
                results['speedup_vs_fp32'].append(speedup)

                # Benchmark FP16 if available
                if self.config.use_fp16:
                    try:
                        _, fp16_time = self.gemm_fp16(A, B)
                        fp16_gflops = operations / (fp16_time * 1e-3 * 1e9)
                        results['fp16_performance'].append(fp16_gflops)
                    except Exception as e:
                        logger.warning(f"FP16 benchmark failed: {e}")
                        results['fp16_performance'].append(0.0)
                else:
                    results['fp16_performance'].append(0.0)

                logger.info(f"Size {size}x{size}: FP32={fp32_gflops:.2f}, "
                           f"Mixed={metrics.gflops_mixed:.2f} GFLOPS, "
                           ".2f"
                           ".3f")

            except Exception as e:
                logger.error(f"Benchmark failed for size {size}x{size}: {e}")
                results['fp32_performance'].append(0.0)
                results['fp16_performance'].append(0.0)
                results['mixed_performance'].append(0.0)
                results['accuracy_loss_percent'].append(100.0)
                results['memory_savings_percent'].append(0.0)
                results['speedup_vs_fp32'].append(0.0)

        # Calculate averages
        results['average_fp32'] = np.mean(results['fp32_performance'])
        results['average_mixed'] = np.mean(results['mixed_performance'])
        results['average_speedup'] = np.mean(results['speedup_vs_fp32'])
        results['average_accuracy_loss'] = np.mean(results['accuracy_loss_percent'])

        logger.info("ðŸ“Š Precision Benchmark Summary:")
        logger.info(f"Average FP32: {results['average_fp32']:.2f} GFLOPS")
        logger.info(f"Average Mixed: {results['average_mixed']:.2f} GFLOPS")
        logger.info(f"Average Speedup: {results['average_speedup']:.2f}x")
        logger.info(f"Average Accuracy Loss: {results['average_accuracy_loss']:.3f}%")

        return results

def main():
    """Main function for testing mixed precision optimizations"""
    logger.info("ðŸš€ Starting Mixed Precision Optimization Test")

    # Initialize mixed precision engine
    engine = MixedPrecisionEngine()

    # Test with different sizes
    test_sizes = [512, 1024]

    for size in test_sizes:
        logger.info(f"Testing {size}x{size} mixed precision GEMM")

        # Generate test matrices
        np.random.seed(42)
        A = np.random.randn(size, size).astype(np.float32)
        B = np.random.randn(size, size).astype(np.float32)

        # Test mixed precision
        C_mixed, metrics = engine.gemm_mixed_precision(A, B)

        # Compare with NumPy reference
        C_numpy = A @ B
        max_error = np.max(np.abs(C_mixed - C_numpy))
        logger.info(f"Max error vs NumPy: {max_error:.2e}")

    # Run comprehensive benchmark
    logger.info("Running comprehensive precision benchmark...")
    benchmark_results = engine.benchmark_precision_performance()

    logger.info("âœ… Mixed precision optimization test completed")

if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
ðŸš€ AI KERNEL PREDICTOR VALIDATION SYSTEM
=======================================

Sistema de validaciÃ³n que prueba la precisiÃ³n del AI Kernel Predictor
contra datos conocidos de Phase 13 y genera mÃ©tricas de rendimiento.

CaracterÃ­sticas:
- ValidaciÃ³n cruzada de predicciones
- ComparaciÃ³n con resultados reales de Phase 13
- MÃ©tricas de precisiÃ³n y error
- AnÃ¡lisis de casos lÃ­mite y edge cases
- Reporte de rendimiento del sistema ML

Author: AI Assistant
Date: 2026-01-25
"""

import numpy as np
import pandas as pd
import json
import os
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union
import logging
import warnings
warnings.filterwarnings('ignore')

# Optional plotting libraries (with fallbacks)
try:
    import matplotlib.pyplot as plt
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False

try:
    import seaborn as sns
    SEABORN_AVAILABLE = True
except ImportError:
    SEABORN_AVAILABLE = False

# ML Libraries
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PredictorValidator:
    """
    Validation System for AI Kernel Predictor

    Tests the accuracy of ML predictions against known Phase 13 results
    and generates comprehensive performance metrics.
    """

    def __init__(self):
        self.base_dir = Path(__file__).parent.parent
        self.data_dir = self.base_dir / "src" / "data"
        self.models_dir = self.base_dir / "src" / "models"
        self.results_dir = self.base_dir / "src" / "validation_results"

        # Create results directory
        self.results_dir.mkdir(parents=True, exist_ok=True)

        # Import the ensemble predictor
        from ensemble_predictor import EnsemblePredictor
        self.predictor = EnsemblePredictor()

        # Load models
        self.predictor.load_models()

        logger.info("ðŸš€ Predictor Validator initialized")

    def load_validation_data(self) -> Dict[str, pd.DataFrame]:
        """Load validation datasets from Phase 13"""
        validation_data = {}

        try:
            # Load the same datasets used for training
            validation_data['workgroup'] = pd.read_csv(self.data_dir / 'phase13_workgroup_dataset.csv')
            validation_data['memory'] = pd.read_csv(self.data_dir / 'phase13_memory_dataset.csv')
            validation_data['combined'] = pd.read_csv(self.data_dir / 'phase13_combined_dataset.csv')
            validation_data['architecture'] = pd.read_csv(self.data_dir / 'phase13_architecture_info.csv')

            logger.info("âœ… Validation datasets loaded")

        except Exception as e:
            logger.error(f"Failed to load validation data: {e}")
            raise

        return validation_data

    def run_comprehensive_validation(self) -> Dict[str, Any]:
        """Run comprehensive validation of all predictor models"""
        logger.info("ðŸ” Running comprehensive validation...")

        # Load validation data
        validation_data = self.load_validation_data()

        # Validate work-group predictions
        wg_results = self._validate_workgroup_predictions(validation_data['workgroup'])

        # Validate memory predictions
        mem_results = self._validate_memory_predictions(validation_data['memory'])

        # Validate combined predictions
        combined_results = self._validate_combined_predictions(validation_data['combined'])

        # Generate overall summary
        summary = self._generate_validation_summary({
            'workgroup': wg_results,
            'memory': mem_results,
            'combined': combined_results
        })

        # Save results
        self._save_validation_results(summary)

        logger.info("âœ… Comprehensive validation completed")
        return summary

    def _validate_workgroup_predictions(self, wg_data: pd.DataFrame) -> Dict[str, Any]:
        """Validate work-group configuration predictions"""
        logger.info("Validating work-group predictions...")

        results = {
            'predictions': [],
            'metrics': {},
            'accuracy_analysis': {},
            'error_analysis': {}
        }

        # Test each work-group configuration
        for idx, row in wg_data.iterrows():
            # Prepare hardware features
            hardware_features = {
                'wg_size_0': row['wg_size_0'],
                'wg_size_1': row['wg_size_1'],
                'wg_total_size': row['wg_total_size'],
                'compute_units': row['compute_units'],
                'wavefront_size': row['wavefront_size'],
                'max_wg_size': row['max_wg_size']
            }

            # Get prediction
            prediction = self.predictor.predict_workgroup_config(hardware_features)

            if 'error' in prediction:
                logger.warning(f"Prediction error for config {idx}: {prediction['error']}")
                continue

            # Compare with actual performance
            actual_gflops = row['gflops']
            predicted_gflops = prediction['predicted_gflops']

            # Calculate errors
            absolute_error = abs(predicted_gflops - actual_gflops)
            relative_error = absolute_error / actual_gflops if actual_gflops > 0 else 0

            # Store prediction result
            result = {
                'config_id': idx,
                'actual_gflops': actual_gflops,
                'predicted_gflops': predicted_gflops,
                'absolute_error': absolute_error,
                'relative_error': relative_error,
                'is_optimal_actual': row['is_optimal'],
                'optimal_probability': prediction['optimal_probability'],
                'prediction_confidence': prediction['confidence'],
                'model_used': prediction['model_used']
            }

            results['predictions'].append(result)

        # Calculate aggregate metrics
        if results['predictions']:
            df_results = pd.DataFrame(results['predictions'])

            results['metrics'] = {
                'mae': float(df_results['absolute_error'].mean()),
                'mape': float(df_results['relative_error'].mean() * 100),  # Mean Absolute Percentage Error
                'rmse': float(np.sqrt(mean_squared_error(df_results['actual_gflops'], df_results['predicted_gflops']))),
                'r2_score': float(r2_score(df_results['actual_gflops'], df_results['predicted_gflops'])),
                'num_predictions': len(df_results),
                'accuracy_threshold_10pct': float((df_results['relative_error'] <= 0.10).mean() * 100),
                'accuracy_threshold_20pct': float((df_results['relative_error'] <= 0.20).mean() * 100)
            }

            # Optimal configuration detection accuracy
            optimal_predictions = df_results[df_results['is_optimal_actual'] == True]
            if len(optimal_predictions) > 0:
                results['accuracy_analysis'] = {
                    'optimal_detection_rate': float((optimal_predictions['optimal_probability'] > 0.5).mean() * 100),
                    'high_confidence_predictions': float((df_results['optimal_probability'] > 0.7).sum()),
                    'total_optimal_configs': len(optimal_predictions)
                }

            # Error distribution analysis
            results['error_analysis'] = {
                'error_percentiles': {
                    '25th': float(df_results['relative_error'].quantile(0.25) * 100),
                    '50th': float(df_results['relative_error'].quantile(0.50) * 100),
                    '75th': float(df_results['relative_error'].quantile(0.75) * 100),
                    '90th': float(df_results['relative_error'].quantile(0.90) * 100)
                },
                'max_error': float(df_results['relative_error'].max() * 100),
                'min_error': float(df_results['relative_error'].min() * 100)
            }

        logger.info(f"Work-group validation: MAE={results['metrics'].get('mae', 0):.2f}, "
                   f"MAPE={results['metrics'].get('mape', 0):.1f}%, "
                   f"RÂ²={results['metrics'].get('r2_score', 0):.3f}")
        return results

    def _validate_memory_predictions(self, mem_data: pd.DataFrame) -> Dict[str, Any]:
        """Validate memory configuration predictions"""
        logger.info("Validating memory predictions...")

        results = {
            'predictions': [],
            'metrics': {},
            'config_analysis': {}
        }

        # Test each memory configuration
        for idx, row in mem_data.iterrows():
            # Prepare hardware features
            hardware_features = {
                'use_lds': row['use_lds'],
                'lds_tile_size': row['lds_tile_size'],
                'vector_width': row['vector_width'],
                'unroll_factor': row['unroll_factor'],
                'prefetch_distance': row['prefetch_distance'],
                'local_mem_size_kb': row['local_mem_size_kb'],
                'global_mem_size_gb': row['global_mem_size_gb']
            }

            # Get prediction
            prediction = self.predictor.predict_memory_config(hardware_features)

            if 'error' in prediction:
                logger.warning(f"Memory prediction error for config {idx}: {prediction['error']}")
                continue

            # Compare with actual performance
            actual_gflops = row['gflops']
            predicted_gflops = prediction['predicted_gflops']

            # Calculate errors
            absolute_error = abs(predicted_gflops - actual_gflops)
            relative_error = absolute_error / actual_gflops if actual_gflops > 0 else 0

            # Store prediction result
            result = {
                'config_id': idx,
                'memory_type': row['memory_type'],
                'actual_gflops': actual_gflops,
                'predicted_gflops': predicted_gflops,
                'absolute_error': absolute_error,
                'relative_error': relative_error,
                'model_used': prediction['model_used']
            }

            results['predictions'].append(result)

        # Calculate aggregate metrics
        if results['predictions']:
            df_results = pd.DataFrame(results['predictions'])

            results['metrics'] = {
                'mae': float(df_results['absolute_error'].mean()),
                'mape': float(df_results['relative_error'].mean() * 100),
                'rmse': float(np.sqrt(mean_squared_error(df_results['actual_gflops'], df_results['predicted_gflops']))),
                'r2_score': float(r2_score(df_results['actual_gflops'], df_results['predicted_gflops'])),
                'num_predictions': len(df_results),
                'accuracy_threshold_15pct': float((df_results['relative_error'] <= 0.15).mean() * 100),
                'accuracy_threshold_25pct': float((df_results['relative_error'] <= 0.25).mean() * 100)
            }

            # Memory type performance analysis
            memory_types = df_results.groupby('memory_type')['relative_error'].agg(['mean', 'std', 'count'])
            results['config_analysis'] = {
                'memory_type_performance': memory_types.to_dict('index'),
                'best_performing_type': memory_types['mean'].idxmin(),
                'worst_performing_type': memory_types['mean'].idxmax()
            }

        logger.info(f"Memory validation: MAE={results['metrics'].get('mae', 0):.2f}, "
                   f"MAPE={results['metrics'].get('mape', 0):.1f}%, "
                   f"RÂ²={results['metrics'].get('r2_score', 0):.3f}")
        return results

    def _validate_combined_predictions(self, combined_data: pd.DataFrame) -> Dict[str, Any]:
        """Validate combined work-group and memory predictions"""
        logger.info("Validating combined predictions...")

        results = {
            'predictions': [],
            'metrics': {},
            'correlation_analysis': {}
        }

        # Test combined configurations (sample to avoid too many predictions)
        sample_size = min(20, len(combined_data))  # Test up to 20 configurations
        test_data = combined_data.sample(n=sample_size, random_state=42)

        for idx, row in test_data.iterrows():
            # Prepare combined hardware features
            hardware_features = {
                'wg_size_0': row['wg_size_0'],
                'wg_size_1': row['wg_size_1'],
                'wg_total_size': row['wg_total_size'],
                'wg_occupancy': row['wg_occupancy'],
                'wg_efficiency': row['wg_efficiency'],
                'use_lds': row['use_lds'],
                'lds_tile_size': row['lds_tile_size'],
                'vector_width': row['vector_width'],
                'unroll_factor': row['unroll_factor'],
                'prefetch_distance': row['prefetch_distance'],
                'compute_units': row['compute_units'],
                'wavefront_size': row['wavefront_size'],
                'local_mem_size_kb': row['local_mem_size_kb'],
                'global_mem_size_gb': row['global_mem_size_gb']
            }

            # Get prediction
            prediction = self.predictor.predict_combined_config(hardware_features)

            if 'error' in prediction:
                logger.warning(f"Combined prediction error for config {idx}: {prediction['error']}")
                continue

            # Compare with actual performance
            actual_gflops = row['estimated_gflops']
            predicted_gflops = prediction['predicted_gflops']

            # Calculate errors
            absolute_error = abs(predicted_gflops - actual_gflops)
            relative_error = absolute_error / actual_gflops if actual_gflops > 0 else 0

            # Store prediction result
            result = {
                'config_id': idx,
                'actual_gflops': actual_gflops,
                'predicted_gflops': predicted_gflops,
                'absolute_error': absolute_error,
                'relative_error': relative_error,
                'is_optimal_combined': row['is_combined_optimal'],
                'model_used': prediction['model_used']
            }

            results['predictions'].append(result)

        # Calculate aggregate metrics
        if results['predictions']:
            df_results = pd.DataFrame(results['predictions'])

            results['metrics'] = {
                'mae': float(df_results['absolute_error'].mean()),
                'mape': float(df_results['relative_error'].mean() * 100),
                'rmse': float(np.sqrt(mean_squared_error(df_results['actual_gflops'], df_results['predicted_gflops']))),
                'r2_score': float(r2_score(df_results['actual_gflops'], df_results['predicted_gflops'])),
                'num_predictions': len(df_results),
                'accuracy_threshold_20pct': float((df_results['relative_error'] <= 0.20).mean() * 100),
                'accuracy_threshold_30pct': float((df_results['relative_error'] <= 0.30).mean() * 100)
            }

            # Correlation analysis between work-group and memory contributions
            wg_contribution = test_data['wg_contribution']
            mem_contribution = test_data['mem_contribution']
            combined_performance = test_data['estimated_gflops']

            wg_corr = float(wg_contribution.corr(combined_performance))
            mem_corr = float(mem_contribution.corr(combined_performance))

            results['correlation_analysis'] = {
                'wg_performance_correlation': wg_corr,
                'memory_performance_correlation': mem_corr,
                'wg_vs_memory_balance': abs(wg_corr - mem_corr)
            }

        logger.info(f"Combined validation: MAE={results['metrics'].get('mae', 0):.2f}, "
                   f"MAPE={results['metrics'].get('mape', 0):.1f}%, "
                   f"RÂ²={results['metrics'].get('r2_score', 0):.3f}")
        return results

    def _generate_validation_summary(self, validation_results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """Generate comprehensive validation summary"""
        summary = {
            'validation_timestamp': pd.Timestamp.now().isoformat(),
            'overall_performance': {},
            'model_comparison': {},
            'recommendations': [],
            'validation_details': validation_results
        }

        # Overall performance metrics
        wg_metrics = validation_results['workgroup'].get('metrics', {})
        mem_metrics = validation_results['memory'].get('metrics', {})
        combined_metrics = validation_results['combined'].get('metrics', {})

        if wg_metrics and mem_metrics and combined_metrics:
            summary['overall_performance'] = {
                'average_mape': float(np.mean([wg_metrics.get('mape', 0),
                                             mem_metrics.get('mape', 0),
                                             combined_metrics.get('mape', 0)])),
                'workgroup_accuracy': wg_metrics.get('accuracy_threshold_10pct', 0),
                'memory_accuracy': mem_metrics.get('accuracy_threshold_15pct', 0),
                'combined_accuracy': combined_metrics.get('accuracy_threshold_20pct', 0),
                'total_predictions': sum([
                    wg_metrics.get('num_predictions', 0),
                    mem_metrics.get('num_predictions', 0),
                    combined_metrics.get('num_predictions', 0)
                ])
            }

        # Model comparison
        summary['model_comparison'] = {
            'workgroup_best_model': validation_results['workgroup'].get('metrics', {}).get('best_model', 'unknown'),
            'memory_best_model': validation_results['memory'].get('metrics', {}).get('best_model', 'unknown'),
            'combined_best_model': validation_results['combined'].get('metrics', {}).get('best_model', 'unknown')
        }

        # Generate recommendations
        recommendations = []

        # Work-group recommendations
        if wg_metrics.get('mape', 100) < 15:
            recommendations.append("âœ… Work-group predictor shows excellent accuracy (<15% MAPE)")
        elif wg_metrics.get('mape', 100) < 25:
            recommendations.append("âš ï¸ Work-group predictor shows good accuracy (15-25% MAPE) - consider fine-tuning")
        else:
            recommendations.append("âŒ Work-group predictor needs improvement (>25% MAPE)")

        # Memory recommendations
        if mem_metrics.get('mape', 100) < 20:
            recommendations.append("âœ… Memory predictor shows good accuracy (<20% MAPE)")
        elif mem_metrics.get('mape', 100) < 30:
            recommendations.append("âš ï¸ Memory predictor shows moderate accuracy (20-30% MAPE)")
        else:
            recommendations.append("âŒ Memory predictor needs significant improvement (>30% MAPE)")

        # Combined recommendations
        if combined_metrics.get('mape', 100) < 25:
            recommendations.append("âœ… Combined predictor shows promising accuracy (<25% MAPE)")
        else:
            recommendations.append("âš ï¸ Combined predictor needs optimization (>25% MAPE)")

        # Overall assessment
        avg_mape = summary['overall_performance'].get('average_mape', 100)
        if avg_mape < 20:
            recommendations.append("ðŸŽ¯ AI Kernel Predictor ready for production use with high confidence")
        elif avg_mape < 30:
            recommendations.append("ðŸ”„ AI Kernel Predictor ready for beta testing with monitoring")
        else:
            recommendations.append("ðŸ“š AI Kernel Predictor needs additional training data and model refinement")

        summary['recommendations'] = recommendations

        return summary

    def _save_validation_results(self, summary: Dict[str, Any]):
        """Save validation results to files"""
        try:
            # Save detailed results
            results_path = self.results_dir / 'validation_results.json'
            with open(results_path, 'w') as f:
                json.dump(summary, f, indent=2)

            # Save summary report
            report_path = self.results_dir / 'validation_report.md'
            self._generate_validation_report(summary, report_path)

            # Save prediction data for analysis
            for category, results in summary['validation_details'].items():
                if 'predictions' in results:
                    df = pd.DataFrame(results['predictions'])
                    csv_path = self.results_dir / f'{category}_predictions.csv'
                    df.to_csv(csv_path, index=False)

            logger.info("âœ… Validation results saved")

        except Exception as e:
            logger.error(f"Failed to save validation results: {e}")

    def _generate_validation_report(self, summary: Dict[str, Any], report_path: Path):
        """Generate human-readable validation report"""
        report = f"""# AI Kernel Predictor Validation Report

**Validation Date:** {summary['validation_timestamp']}

## Executive Summary

The AI Kernel Predictor has been validated against Phase 13 optimization results from the Radeon RX 580 GCN architecture tuning project.

### Overall Performance
- **Average MAPE:** {summary['overall_performance'].get('average_mape', 'N/A'):.1f}%
- **Total Predictions Tested:** {summary['overall_performance'].get('total_predictions', 0)}
- **Work-group Accuracy (â‰¤10% error):** {summary['overall_performance'].get('workgroup_accuracy', 0):.1f}%
- **Memory Accuracy (â‰¤15% error):** {summary['overall_performance'].get('memory_accuracy', 0):.1f}%
- **Combined Accuracy (â‰¤20% error):** {summary['overall_performance'].get('combined_accuracy', 0):.1f}%

## Detailed Results

### Work-Group Predictions
"""

        wg_metrics = summary['validation_details']['workgroup'].get('metrics', {})
        if wg_metrics:
            report += f"""- **MAE:** {wg_metrics.get('mae', 0):.2f} GFLOPS
- **MAPE:** {wg_metrics.get('mape', 0):.1f}%
- **RÂ² Score:** {wg_metrics.get('r2_score', 0):.3f}
- **Accuracy â‰¤10%:** {wg_metrics.get('accuracy_threshold_10pct', 0):.1f}%
- **Accuracy â‰¤20%:** {wg_metrics.get('accuracy_threshold_20pct', 0):.1f}%

"""

        mem_metrics = summary['validation_details']['memory'].get('metrics', {})
        if mem_metrics:
            report += f"""### Memory Predictions
- **MAE:** {mem_metrics.get('mae', 0):.2f} GFLOPS
- **MAPE:** {mem_metrics.get('mape', 0):.1f}%
- **RÂ² Score:** {mem_metrics.get('r2_score', 0):.3f}
- **Accuracy â‰¤15%:** {mem_metrics.get('accuracy_threshold_15pct', 0):.1f}%
- **Accuracy â‰¤25%:** {mem_metrics.get('accuracy_threshold_25pct', 0):.1f}%

"""

        combined_metrics = summary['validation_details']['combined'].get('metrics', {})
        if combined_metrics:
            report += f"""### Combined Predictions
- **MAE:** {combined_metrics.get('mae', 0):.2f} GFLOPS
- **MAPE:** {combined_metrics.get('mape', 0):.1f}%
- **RÂ² Score:** {combined_metrics.get('r2_score', 0):.3f}
- **Accuracy â‰¤20%:** {combined_metrics.get('accuracy_threshold_20pct', 0):.1f}%
- **Accuracy â‰¤30%:** {combined_metrics.get('accuracy_threshold_30pct', 0):.1f}%

"""

        report += f"""## Recommendations

"""
        for rec in summary['recommendations']:
            report += f"- {rec}\n"

        report += f"""

## Technical Details

### Models Used
- **Work-group:** {str(summary['model_comparison'].get('workgroup_best_model', 'N/A'))}
- **Memory:** {str(summary['model_comparison'].get('memory_best_model', 'N/A'))}
- **Combined:** {str(summary['model_comparison'].get('combined_best_model', 'N/A'))}

### Validation Methodology
- Cross-validation against Phase 13 benchmark results
- Hardware: AMD Radeon RX 580 (GCN 4.0, 36 compute units)
- Test matrix sizes: 1024x1024
- Performance metric: GFLOPS sustained

---
*Report generated automatically by AI Kernel Predictor Validation System*
"""

        with open(report_path, 'w') as f:
            f.write(report)

def main():
    """Main function for predictor validation"""
    try:
        validator = PredictorValidator()

        # Run comprehensive validation
        results = validator.run_comprehensive_validation()

        # Print summary
        perf = results.get('overall_performance', {})
        logger.info("ðŸŽ¯ Validation completed successfully!")
        logger.info(f"ðŸ“Š Average MAPE: {perf.get('average_mape', 0):.1f}%")
        logger.info(f"ðŸ“Š Total predictions tested: {perf.get('total_predictions', 0)}")
        logger.info(f"ðŸ“ˆ Work-group accuracy: {perf.get('workgroup_accuracy', 0):.1f}%")
        logger.info(f"ðŸ“ˆ Memory accuracy: {perf.get('memory_accuracy', 0):.1f}%")
        logger.info(f"ðŸ“ˆ Combined accuracy: {perf.get('combined_accuracy', 0):.1f}%")

        # Print recommendations
        logger.info("ðŸ’¡ Key Recommendations:")
        for rec in results.get('recommendations', []):
            logger.info(f"   {rec}")

    except Exception as e:
        logger.error(f"Validation failed: {e}")
        raise

if __name__ == "__main__":
    main()
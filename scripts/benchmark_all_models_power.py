#!/usr/bin/env python3
"""
Benchmark All Models with Power Measurements
==============================================

Comprehensive benchmark of all major models with real power monitoring.
Generates data for academic paper.

This script:
1. Benchmarks ResNet-18, ResNet-50, MobileNetV2, etc.
2. Tests FP32, FP16, INT8 quantizations
3. Measures real power consumption
4. Calculates efficiency metrics
5. Generates comparison tables

Usage:
    python scripts/benchmark_all_models_power.py --output results/power_benchmarks.json
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import argparse
import json
import time
from datetime import datetime
from typing import Dict, List

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

from src.profiling.power_profiler import BenchmarkWithPower, BenchmarkResults
from scripts.power_monitor import GPUPowerMonitor


class ModelFactory:
    """Create test models."""
    
    @staticmethod
    def create_simple_cnn():
        """Simple CNN for quick testing."""
        import torch.nn.functional as F
        
        class SimpleCNN(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
                self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
                self.pool = nn.MaxPool2d(2, 2)
                self.fc1 = nn.Linear(64 * 8 * 8, 128)
                self.fc2 = nn.Linear(128, 10)
            
            def forward(self, x):
                x = self.pool(F.relu(self.conv1(x)))
                x = self.pool(F.relu(self.conv2(x)))
                x = x.reshape(x.size(0), -1)
                x = F.relu(self.fc1(x))
                return self.fc2(x)
        
        return SimpleCNN()
    
    @staticmethod
    def create_resnet18():
        """ResNet-18."""
        try:
            from torchvision.models import resnet18
            return resnet18(pretrained=False)
        except ImportError:
            print("‚ö†Ô∏è  torchvision not available, using SimpleCNN instead")
            return ModelFactory.create_simple_cnn()
    
    @staticmethod
    def create_mobilenetv2():
        """MobileNetV2."""
        try:
            from torchvision.models import mobilenet_v2
            return mobilenet_v2(pretrained=False)
        except ImportError:
            print("‚ö†Ô∏è  torchvision not available, using SimpleCNN instead")
            return ModelFactory.create_simple_cnn()


def create_test_dataloader(image_size=224, batch_size=32, num_samples=500):
    """Create test data loader."""
    X = torch.randn(num_samples, 3, image_size, image_size)
    y = torch.randint(0, 10, (num_samples,))
    dataset = TensorDataset(X, y)
    return DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)


class ModelBenchmarker:
    """Benchmark multiple models with power monitoring."""
    
    def __init__(self, duration: int = 30, warmup: int = 5):
        """
        Initialize benchmarker.
        
        Args:
            duration: Benchmark duration per model
            warmup: Warmup duration
        """
        self.duration = duration
        self.warmup = warmup
        self.results = {}
    
    def benchmark_model(
        self,
        name: str,
        model: nn.Module,
        data_loader: DataLoader,
        quantization: str = "FP32",
        device: str = "cpu"
    ) -> Dict:
        """
        Benchmark a single model.
        
        Args:
            name: Model name
            model: PyTorch model
            data_loader: Test data
            quantization: Quantization type
            device: Device to run on ('cpu' or 'cuda')
            
        Returns:
            Dictionary with results
        """
        print(f"\n{'='*70}")
        print(f"Benchmarking: {name} ({quantization}) on {device.upper()}")
        print(f"{'='*70}")
        
        try:
            # Move model to device
            model = model.to(device)
            # Run benchmark
            benchmark = BenchmarkWithPower(model, data_loader, verbose=False)
            results = benchmark.run(duration=self.duration, warmup_seconds=self.warmup)
            
            # Count parameters
            num_params = sum(p.numel() for p in model.parameters())
            
            # Compile results
            result_dict = {
                'model_name': name,
                'quantization': quantization,
                'num_parameters': num_params,
                'duration_seconds': results.duration_seconds,
                'num_inferences': results.num_inferences,
                'fps': results.fps,
                'avg_latency_ms': results.avg_latency_ms,
                'avg_power_watts': results.avg_power_watts,
                'min_power_watts': results.min_power_watts,
                'max_power_watts': results.max_power_watts,
                'total_energy_joules': results.total_energy_joules,
                'energy_per_inference_joules': results.energy_per_inference_joules,
                'fps_per_watt': results.fps_per_watt,
                'inferences_per_joule': results.inferences_per_joule,
                'avg_temperature': results.avg_temperature,
                'timestamp': datetime.now().isoformat()
            }
            
            # Print summary
            print(f"\n‚úÖ Results:")
            print(f"  Parameters:    {num_params:,}")
            print(f"  FPS:           {results.fps:.2f}")
            print(f"  Latency:       {results.avg_latency_ms:.2f} ms")
            print(f"  Avg Power:     {results.avg_power_watts:.2f} W")
            print(f"  Energy/Img:    {results.energy_per_inference_joules*1000:.2f} mJ")
            print(f"  FPS/Watt:      {results.fps_per_watt:.2f}")
            
            return result_dict
            
        except Exception as e:
            print(f"‚ùå Error benchmarking {name}: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def benchmark_all_models(
        self,
        models: List[tuple],
        batch_size: int = 32,
        device: str = "cpu"
    ):
        """
        Benchmark all models.
        
        Args:
            models: List of (name, model_fn, image_size) tuples
            batch_size: Batch size for testing
            device: Device to run on ('cpu' or 'cuda')
        """
        all_results = []
        
        print(f"\n{'='*70}")
        print(f"Benchmarking {len(models)} models on {device.upper()}")
        print(f"Duration: {self.duration}s per model (+ {self.warmup}s warmup)")
        print(f"Batch size: {batch_size}")
        print(f"{'='*70}")
        
        for i, (name, model_fn, image_size) in enumerate(models, 1):
            print(f"\n[{i}/{len(models)}] {name}")
            
            try:
                # Create model and data
                model = model_fn()
                data_loader = create_test_dataloader(
                    image_size=image_size,
                    batch_size=batch_size,
                    num_samples=500
                )
                
                # Benchmark
                result = self.benchmark_model(name, model, data_loader, device=device)
                
                if result:
                    all_results.append(result)
                
                # Small delay between models
                time.sleep(2)
                
            except Exception as e:
                print(f"‚ùå Failed: {e}")
                continue
        
        return all_results
    
    def generate_comparison_table(self, results: List[Dict]) -> str:
        """Generate markdown comparison table."""
        if not results:
            return "No results to compare"
        
        table = "\n## Model Comparison - AMD Radeon RX 580\n\n"
        table += "| Model | Quantization | Params | FPS | Power (W) | FPS/W | Energy/Img (mJ) |\n"
        table += "|-------|--------------|--------|-----|-----------|-------|------------------|\n"
        
        for r in results:
            table += f"| {r['model_name']} | "
            table += f"{r['quantization']} | "
            table += f"{r['num_parameters']:,} | "
            table += f"{r['fps']:.1f} | "
            table += f"{r['avg_power_watts']:.1f} | "
            table += f"{r['fps_per_watt']:.2f} | "
            table += f"{r['energy_per_inference_joules']*1000:.2f} |\n"
        
        return table
    
    def save_results(self, results: List[Dict], filename: str):
        """Save results to JSON file."""
        output = {
            'timestamp': datetime.now().isoformat(),
            'system_info': {
                'gpu': 'AMD Radeon RX 580',
                'cuda_available': torch.cuda.is_available()
            },
            'benchmark_config': {
                'duration_seconds': self.duration,
                'warmup_seconds': self.warmup
            },
            'results': results
        }
        
        os.makedirs(os.path.dirname(filename) if os.path.dirname(filename) else '.', exist_ok=True)
        
        with open(filename, 'w') as f:
            json.dump(output, f, indent=2)
        
        print(f"\nüíæ Results saved to: {filename}")


def main():
    """Main function."""
    parser = argparse.ArgumentParser(description='Benchmark models with power monitoring')
    parser.add_argument('--duration', type=int, default=30, help='Benchmark duration per model (seconds)')
    parser.add_argument('--warmup', type=int, default=5, help='Warmup duration (seconds)')
    parser.add_argument('--batch-size', type=int, default=32, help='Batch size')
    parser.add_argument('--output', type=str, default='results/power_benchmarks.json', help='Output JSON file')
    parser.add_argument('--models', type=str, default='all', help='Models to benchmark: all, simple, standard')
    parser.add_argument('--device', type=str, default='cpu', help='Device to run on: cpu or cuda')
    
    args = parser.parse_args()
    
    # Check device availability
    if args.device == 'cuda' and not torch.cuda.is_available():
        print("‚ùå CUDA not available. Falling back to CPU.")
        args.device = 'cpu'
    else:
        print(f"\n‚úÖ Using device: {args.device.upper()}")
        if args.device == 'cuda':
            print(f"   GPU: {torch.cuda.get_device_name(0)}")
    
    # Define models to benchmark
    if args.models == 'simple':
        models = [
            ('SimpleCNN', ModelFactory.create_simple_cnn, 32)
        ]
    elif args.models == 'standard':
        models = [
            ('ResNet-18', ModelFactory.create_resnet18, 224),
            ('MobileNetV2', ModelFactory.create_mobilenetv2, 224)
        ]
    else:  # all
        models = [
            ('SimpleCNN', ModelFactory.create_simple_cnn, 32),
            ('ResNet-18', ModelFactory.create_resnet18, 224),
            ('MobileNetV2', ModelFactory.create_mobilenetv2, 224)
        ]
    
    # Check if power monitoring is available
    try:
        monitor = GPUPowerMonitor(verbose=True)
        print(f"\n‚úÖ Power monitoring ready (method: {monitor.method})")
    except Exception as e:
        print(f"\n‚ùå Power monitoring not available: {e}")
        print("   Continuing without power measurements...")
    
    # Run benchmarks
    benchmarker = ModelBenchmarker(duration=args.duration, warmup=args.warmup)
    results = benchmarker.benchmark_all_models(models, batch_size=args.batch_size, device=args.device)
    
    # Generate comparison table
    table = benchmarker.generate_comparison_table(results)
    print(table)
    
    # Save results
    if results:
        benchmarker.save_results(results, args.output)
        
        # Also save markdown table
        md_file = args.output.replace('.json', '.md')
        with open(md_file, 'w') as f:
            f.write(table)
        print(f"üìä Comparison table saved to: {md_file}")
    
    print(f"\n{'='*70}")
    print("Benchmark complete!")
    print(f"{'='*70}\n")


if __name__ == '__main__':
    main()

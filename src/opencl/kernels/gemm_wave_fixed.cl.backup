/**
 * GCN 4.0 Wave-Level Optimized GEMM Kernel - DOUBLE BUFFERED + MEMORY COALESCING
 * Integrates wave-level and hybrid optimizations for Polaris 10
 *
 * Target: 1000+ GFLOPS (29% improvement over Phase 1 baseline)
 * Hardware: AMD Radeon RX 590 (Polaris 10, GCN 4.0)
 *
 * Optimizations:
 * - Workgroup (16,16) = 256 threads (optimal occupancy)
 * - Tile size 16x16 with LDS padding (bank conflict avoidance)
 * - Double buffering for latency hiding (compute/communicate overlap)
 * - Memory coalescing for 128-byte aligned global memory access
 * - Coalesced memory access patterns
 * - Loop unrolling for maximum ILP
 * - Corrected boundary handling
 *
 * Memory Coalescing Strategy:
 * - Threads load consecutive elements in global memory
 * - 128-byte aligned access patterns for optimal bandwidth
 * - Minimized cache misses and memory transactions
 *
 * Performance Status:
 * - Current: ~68 GFLOPS (256x256x256 matrices)
 * - Target: 200-300 GFLOPS (intermediate milestone)
 * - Final: 1000+ GFLOPS with additional GCN optimizations
 */

#ifndef TILE_SIZE
#define TILE_SIZE 16
#endif

#ifndef WG_SIZE_X
#define WG_SIZE_X 16
#endif

#ifndef WG_SIZE_Y
#define WG_SIZE_Y 16
#endif

#ifndef LDS_PADDING
#define LDS_PADDING 2  // 8 bytes (2 floats) for bank conflict avoidance
#endif

/**
 * GCN 4.0 Double-Buffered Optimized GEMM Kernel
 *
 * Double buffering technique:
 * - Two LDS buffers per tile type (A and B) for overlapping compute and load
 * - Pipeline: Load tile N+1 while computing tile N
 * - Reduces memory latency impact on overall performance
 *
 * Performance Notes:
 * - Float4 vectorization attempted but showed performance regression
 * - Current implementation optimized for Polaris GCN 4.0 architecture
 * - Focus on latency hiding and memory coalescing
 *
 * Workgroup: (16,16) = 256 threads
 * Tile: 16x16 elements
 * LDS Usage: 4×(16×18×4) = 4.6 KB per workgroup (2 buffers × 2 tiles)
 * Performance: ~68 GFLOPS current, target 1000+ GFLOPS
 */
__kernel void gemm_wave_optimized(
    const int M,
    const int N,
    const int K,
    const float alpha,
    const float beta,
    __global const float* A,
    __global const float* B,
    __global float* C)
{
    // Workgroup position
    const int wg_x = get_group_id(0);
    const int wg_y = get_group_id(1);

    // Local thread position within workgroup
    const int local_x = get_local_id(0);
    const int local_y = get_local_id(1);
    const int local_id = local_y * WG_SIZE_X + local_x;

    // Global thread position
    const int global_row = wg_x * TILE_SIZE + local_y;
    const int global_col = wg_y * TILE_SIZE + local_x;

    // Each thread computes one element of C using float4 vectorization
    // Accumulator for this thread's output element
    float c = 0.0f;

    // Double-buffered LDS memory for tiles with padding (bank conflict avoidance)
    // Buffer 0 and 1 for overlapping compute and load operations
    __local float A_tile[2][TILE_SIZE][TILE_SIZE + LDS_PADDING];
    __local float B_tile[2][TILE_SIZE][TILE_SIZE + LDS_PADDING];

    // Number of tiles to process
    const int num_tiles = (K + TILE_SIZE - 1) / TILE_SIZE;

    // ============================================================================
    // DOUBLE BUFFERING PIPELINE WITH MEMORY COALESCING
    // ============================================================================

    // Load first tile into buffer 0 with coalesced memory access
    int tile_k = 0;
    {
        // ========================================================================
        // COALESCED LOADING STRATEGY FOR POLARIS 10
        // ========================================================================
        // Each thread loads consecutive elements to maximize memory coalescing
        // Pattern: Threads load entire rows/columns in coalesced fashion

        // Load A tile - coalesced row-wise loading
        // Each thread loads one entire row of the tile
        const int row_to_load = local_y;  // Each thread loads one row
        const int global_a_row = wg_x * TILE_SIZE + row_to_load;
        if (global_a_row < M) {
            for (int col = 0; col < TILE_SIZE; col++) {
                const int global_a_col = tile_k + col;
                if (global_a_col < K) {
                    A_tile[0][row_to_load][col] = A[global_a_row * K + global_a_col];
                } else {
                    A_tile[0][row_to_load][col] = 0.0f;
                }
            }
        } else {
            for (int col = 0; col < TILE_SIZE; col++) {
                A_tile[0][row_to_load][col] = 0.0f;
            }
        }

        // Load B tile - coalesced column-wise loading
        // Each thread loads one entire column of the tile
        const int col_to_load = local_x;  // Each thread loads one column
        const int global_b_col = wg_y * TILE_SIZE + col_to_load;
        if (global_b_col < N) {
            for (int row = 0; row < TILE_SIZE; row++) {
                const int global_b_row = tile_k + row;
                if (global_b_row < K) {
                    B_tile[0][row][col_to_load] = B[global_b_row * N + global_b_col];
                } else {
                    B_tile[0][row][col_to_load] = 0.0f;
                }
            }
        } else {
            for (int row = 0; row < TILE_SIZE; row++) {
                B_tile[0][row][col_to_load] = 0.0f;
            }
        }
    }

    // Synchronize after loading first tile
    barrier(CLK_LOCAL_MEM_FENCE);

    // Main computation loop with double buffering
    for (int t = 0; t < num_tiles; t++) {
        // Current buffer index (0 or 1)
        const int buf_idx = t % 2;

        // Next tile index for prefetching
        const int next_t = t + 1;
        const int next_tile_k = next_t * TILE_SIZE;

        // ========================================================================
        // PHASE 1: COMPUTE with current buffer (optimized for Polaris)
        // ========================================================================

        // Compute partial dot product using current buffer
        // Unroll fully for maximum ILP on GCN architecture
        #pragma unroll 16
        for (int k = 0; k < TILE_SIZE; k++) {
            c += A_tile[buf_idx][local_y][k] * B_tile[buf_idx][k][local_x];
        }

        // ========================================================================
        // PHASE 2: PREFETCH next tile into the other buffer (if not last iteration)
        // ========================================================================

        if (next_t < num_tiles) {
            // Determine which buffer to load into (opposite of current)
            const int load_buf_idx = (buf_idx + 1) % 2;

            // ====================================================================
            // COALESCED PREFETCHING FOR NEXT TILE
            // ====================================================================

            // Load next A tile - coalesced row-wise loading
            const int row_to_load = local_y;
            const int global_a_row = wg_x * TILE_SIZE + row_to_load;
            if (global_a_row < M) {
                for (int col = 0; col < TILE_SIZE; col++) {
                    const int global_a_col = next_tile_k + col;
                    if (global_a_col < K) {
                        A_tile[load_buf_idx][row_to_load][col] = A[global_a_row * K + global_a_col];
                    } else {
                        A_tile[load_buf_idx][row_to_load][col] = 0.0f;
                    }
                }
            } else {
                for (int col = 0; col < TILE_SIZE; col++) {
                    A_tile[load_buf_idx][row_to_load][col] = 0.0f;
                }
            }

            // Load next B tile - coalesced column-wise loading
            const int col_to_load = local_x;
            const int global_b_col = wg_y * TILE_SIZE + col_to_load;
            if (global_b_col < N) {
                for (int row = 0; row < TILE_SIZE; row++) {
                    const int global_b_row = next_tile_k + row;
                    if (global_b_row < K) {
                        B_tile[load_buf_idx][row][col_to_load] = B[global_b_row * N + global_b_col];
                    } else {
                        B_tile[load_buf_idx][row][col_to_load] = 0.0f;
                    }
                }
            } else {
                for (int row = 0; row < TILE_SIZE; row++) {
                    B_tile[load_buf_idx][row][col_to_load] = 0.0f;
                }
            }
        }

        // Synchronize before next iteration (ensure load is complete)
        barrier(CLK_LOCAL_MEM_FENCE);
    }

    // Write result back to global memory
    if (global_row < M && global_col < N) {
        // Apply alpha/beta scaling
        float result = alpha * c;
        if (beta != 0.0f) {
            result += beta * C[global_row * N + global_col];
        }
        C[global_row * N + global_col] = result;
    }
}

/**
 * Double-buffered version for advanced overlapping
 * Currently simplified - can be enhanced later
 */
__kernel void gemm_wave_double_buffered(
    const int M,
    const int N,
    const int K,
    const float alpha,
    const float beta,
    __global const float* A,
    __global const float* B,
    __global float* C)
{
    // For now, use the optimized single-buffered version
    // Double buffering can be implemented later for further optimization
    gemm_wave_optimized(M, N, K, alpha, beta, A, B, C);
}